# ğŸ§  Attention Is All You Need â€” Transformer from Scratch

A full PyTorch implementation of the Transformer architecture proposed in the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper by Vaswani et al., built **entirely from scratch** â€” no `nn.Transformer` used. This repo walks through **core components** like Multi-Head Attention, Positional Encoding, Residual Connections, and builds a full encoder-decoder architecture.

---

## ğŸ“Œ Table of Contents

- [ğŸ“– Background](#-background)
- [ğŸ›  Features](#-features)
- [ğŸ“‚ Project Structure](#-project-structure)
- [ğŸš€ How to Run](#-how-to-run)
- [ğŸ“ˆ Training Phase](#-training-phase)
- [ğŸ“Š Results](#-results)
- [ğŸ“š References](#-references)

---

## ğŸ“– Background

> â€œAttention is all you needâ€ introduced a novel, fully attention-based architecture that eliminated recurrence entirely in sequence transduction tasks.

This project recreates that model step by step using only **PyTorch base modules**.

Key ideas:
- Multi-Head Scaled Dot-Product Attention
- Positional Encoding (sinusoidal)
- Layer Normalization & Residual Connections
- Encoder-Decoder Structure

---

## ğŸ›  Features

âœ… Pure PyTorch implementation (no high-level `nn.Transformer`)  
âœ… Modular, extensible architecture  
âœ… Toy `CopyDataset` for sanity checking  
âœ… Supports batch training, masking, and auto-regressive decoding  
âœ… Well-structured for later expansion to translation, summarization, etc.

---

## ğŸ“‚ Project Structure

```bash
transformer_project/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dataset.py              # Copy task dataset
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ transformer.py          # Transformer architecture
â”‚   â””â”€â”€ transformer_modules.py  # MHA, FFN, PosEnc, etc.
â”œâ”€â”€ train/
â”‚   â””â”€â”€ train_loop.py           # Training logic (if separated)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ config.py               # Hyperparameters
â”‚   â””â”€â”€ masks.py                # Attention masks
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                # Run training
â”‚   â””â”€â”€ evaluate.py             # (Coming soon) Test predictions
â””â”€â”€ README.md
```

---

## ğŸš€ How to Run

### âœ… 1. Set up environment

```bash
conda create -n attention-fixed python=3.9
conda activate attention-fixed
pip install torch numpy
```

### âœ… 2. Train the model

```bash
python scripts/train.py
```

### âœ… 3. Evaluate on custom inputs *(after `evaluate.py` is added)*

---

## ğŸ“ˆ Training Phase

The model is trained on a simple **CopyDataset** â€” a synthetic task where the target is to exactly match the input sequence. This is a standard sanity check for sequence-to-sequence models.

The loss over 10 epochs is shown below:

```
Epoch 1: Loss = 65.6407
Epoch 2: Loss = 45.7510
Epoch 3: Loss = 41.2413
Epoch 4: Loss = 38.0328
Epoch 5: Loss = 35.7573
Epoch 6: Loss = 31.8876
Epoch 7: Loss = 24.8849
Epoch 8: Loss = 18.8855
Epoch 9: Loss = 14.8475
Epoch 10: Loss = 11.7760
```

This decreasing trend in cross-entropy loss demonstrates that the model is successfully learning the identity function, i.e., copying the input sequence to the output.

---

## ğŸ“Š Results

Sample loss curve on CopyDataset (vocab size = 10, seq len = 7):

| Epoch | Loss     |
|-------|----------|
| 1     | 65.64    |
| 5     | 35.75    |
| 10    | 11.77    |

ğŸ‘‰ Model is able to reliably learn the identity function (copying inputs).

---

## ğŸ“š References

- [Attention is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)
- [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [The Annotated Transformer (Harvard NLP)](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
